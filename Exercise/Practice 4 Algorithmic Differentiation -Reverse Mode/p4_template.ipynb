{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Differentiation -- Reverse Mode\n",
    "\n",
    "In this exercise, we want to show you how algorithmic differentiation in reverse mode works.\n",
    "To do this, we show you, how this can be done with python and numpy.\n",
    "\n",
    "Your key takehome message is, that you learn how neuronal network frameworks work.\n",
    "Most frameworks hide the code that you will see in this notebook in 10 000s of lines of code.\n",
    "So it is difficult to get an idea of what they do, first because of the number of code lines and second because their implementations are usually quite complicated.\n",
    "The code here is mainly inspired by chainer, CNTK, and autograd.\n",
    "Pytorch is also not to far away and tensorflow 2 uses a similar user API.\n",
    "\n",
    "While the implementation of the \"core\" of algorithmic differentiation gives you an idea of how neuronal network frameworks work in general, the later application will give you an idea for the programming pattern of all mentioned frameworks.\n",
    "\n",
    "What is your task:\n",
    " - Read the text and try to get an idea what the code should do (You may take later a closer look at these code blocks)\n",
    " - Answer the questions for you.\n",
    " - Search for `???` in the code. They indicate that you have to do something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the base class of all further operations (add, subtract, ...) and the a class that stores the values.\n",
    "Here, it is defined that way, that the execution of the operation directly takes place,\n",
    "once we initialize the `Operation` object and return a `Variable` instance instead of the `Operation` instance:\n",
    "\n",
    "```python\n",
    "output_variable = Operation(input_variable_1, input_variable_2)\n",
    "```\n",
    "\n",
    "This is all written in the `__new__` method. It also takes care of storing all\n",
    "input arguments (`args`, `kwargs`). The output value (`value`) is stored in the `Variable` object.\n",
    "\n",
    "The key methods are:\n",
    "- `apply`: The execution of the operation, i.e. sum two variables in the `Add` operation.\n",
    "- `grad`: Calculates the gradiens, when we want to do back-propagation later.\n",
    "\n",
    "The `Variable` object stores a `value` and the `Operation` instance that produced that value in the instantiation (`__init__`).\n",
    "Since the `Operation` knows its positional arguments (`args`) and keyword arguments (`kwargs`)\n",
    "the complete history of the computation is known.\n",
    "\n",
    "The remaining methodes of the `Variable` class just registers operations,\n",
    "such that we can write `a + b` instead of `Add(a, b)`.\n",
    "\n",
    "For now, you can skip the following code block. \n",
    "You can later take a deeper look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Get a new class instance\n",
    "        operation = super().__new__(cls)\n",
    "        operation.__init__()\n",
    "        \n",
    "        # args are differentiable\n",
    "        operation.args = [a if hasattr(a, 'value') else Input(a) for a in args]\n",
    "        \n",
    "        # Not differentiable\n",
    "        for key, value in kwargs.items():\n",
    "            assert not isinstance(value, Variable), (key, value)\n",
    "        operation.kwargs = kwargs\n",
    "\n",
    "        def as_numpy(a):\n",
    "            if hasattr(a, 'value'):\n",
    "                return a.value\n",
    "            elif isinstance(a, (int, float, complex)):\n",
    "                return np.asanyarray(a)\n",
    "            else:\n",
    "                return a\n",
    "\n",
    "        value = np.array(operation.apply(\n",
    "            *[as_numpy(a) for a in args],\n",
    "            **{k: as_numpy(v) for k, v in kwargs.items()},\n",
    "        ))\n",
    "\n",
    "        assert value.dtype != np.object, (operation, value, value.dtype)\n",
    "        return Variable(value=value, operation=operation)\n",
    "\n",
    "    def apply(self, *args, **kwargs):\n",
    "        # NOTE: This is an error message.\n",
    "        #       You do not need to change something here.\n",
    "        #       This message later tries to help you,\n",
    "        #       when you forgot to implement the apply method.\n",
    "        raise NotImplementedError(f'ToDo: implement the apply method for {type(self)}.')\n",
    "\n",
    "    def grad(self, gz):\n",
    "        # NOTE: This is an error message.\n",
    "        #       You do not need to change something here.\n",
    "        #       This message later tries to help you,\n",
    "        #       when you forgot to implement the apply method.\n",
    "        raise NotImplementedError(f'ToDo: implement the grad method for {type(self)}.')\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'<{self.__class__.__qualname__}>'\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, operation):\n",
    "        self.value = np.array(value)\n",
    "        self.operation = operation\n",
    "    \n",
    "    def backprop(self):\n",
    "        # We define the backpropagation code later.\n",
    "        return backprop(self)\n",
    "    \n",
    "    # ------------------------------------------------------ #\n",
    "    # You can ignore the code below, it only defines python  #\n",
    "    # magic functions to write 'a + b' instead of Add(a, b)  #\n",
    "    # ------------------------------------------------------ #\n",
    "        \n",
    "    def __add__(self, other):                 return Add(self, other)\n",
    "    def __radd__(self, other):                return Add(other, self)\n",
    "    def __sub__(self, other):                 return Sub(self, other)\n",
    "    def __rsub__(self, other):                return Sub(other, self)\n",
    "    def __mul__(self, other):                 return Mul(self, other)\n",
    "    def __rmul__(self, other):                return Mul(other, self)\n",
    "    def __truediv__(self, other):             raise NotImplementedError()\n",
    "    def __rtruediv__(self, other):            raise NotImplementedError()\n",
    "    def __matmul__(self, other):              return MatMul(self, other)\n",
    "    def __rmatmul__(self, other):             return MatMul(other, self)\n",
    "    def __getitem__(self, item):              return GetItem(self, item)\n",
    "    def sum(self, axis=None, keepdims=False): return Sum(self, axis=axis, keepdims=keepdims)\n",
    "    def __neg__(self):                        raise NotImplementedError()\n",
    "    def __pow__(self, other):                 return Pow(self, other)\n",
    "    \n",
    "    # With \"__array_prioriy__ > 0\" and an arithmetic operation with an numpy\n",
    "    # ndarray (e.g. array + variable) calls \"Variable.__radd__\" instead of\n",
    "    # ndarray.__add__.\n",
    "    # https://docs.scipy.org/doc/numpy/reference/arrays.classes.html#numpy.class.__array_priority__\n",
    "    __array_priority__ = 100\n",
    "    \n",
    "    def __repr__(self):\n",
    "        def string_function(a):\n",
    "            if a.size < 20:\n",
    "                return str(a)\n",
    "            else:\n",
    "                return f'array(shape={a.shape}, dtype={a.dtype})'\n",
    "\n",
    "        args = f'{string_function(self.value)}'\n",
    "        if self.operation is not None:\n",
    "            args += f', {self.operation}'\n",
    "            \n",
    "        return f'{self.__class__.__qualname__}({args})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest operation is the identity operation.\n",
    "We provide the complete code here such that all other operations can be implemented by imitating this code.\n",
    "\n",
    "One interesting bit is, that we define the `grad()` function within the `apply()` function.\n",
    "Although this may seem odd in the beginning, it allows us to access the variables which were calculated during the `apply()` function.\n",
    "If you are unfamiliar with this concept, you may want to read about [Nested Functions](https://en.wikibooks.org/wiki/Python_Programming/Functions#Nested_functions) and [Closures](https://en.wikibooks.org/wiki/Python_Programming/Functions#Closures).\n",
    "\n",
    "But be careful, the output of the `grad()` function is a tuple of as many elements as the input to the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Operation):\n",
    "    def apply(self, x):\n",
    "        z = x\n",
    "\n",
    "        def grad(gz):\n",
    "            gx = gz\n",
    "            return gx,\n",
    "        self.grad = grad\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learnable parameters and inputs we could use the `Variable` class.\n",
    "For convenience, let us define aliases for the `Variable` class:\n",
    "The `Parameter` class and the `Input` class. We will later interprete instances of these classes as inputs or learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used for Variables that are learnable.\n",
    "    You can later use this class to distinguish learnable variables\n",
    "    from other variables (`isinstance(variable, Parameter)`).\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)\n",
    "        \n",
    "class Input(Variable):\n",
    "    \"\"\"\n",
    "    This class should be used as wrapper for inputs that are not learnable.\n",
    "    \"\"\"\n",
    "    def __init__(self, value):\n",
    "        super().__init__(value, operation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect, what we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Identity(Parameter(5.))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.operation.grad(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to handle some broadcasting details. You do not need to read this code to understand the remaining part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_broadcast(input_operation, gradient, current_operation):\n",
    "    # Revert broadcast for singleton dimensions\n",
    "    shape = input_operation.value.shape\n",
    "    axis = tuple([\n",
    "        -axis\n",
    "        for axis, (i, j) in enumerate(zip(shape[::-1], gradient.shape[::-1]), 1)\n",
    "        if i != j or j is None\n",
    "    ])\n",
    "    if len(axis) > 0:\n",
    "        gradient = np.sum(gradient, axis=axis, keepdims=True)\n",
    "\n",
    "    # Revert broadcast for independent dims\n",
    "    axis = tuple([*range(len(gradient.shape) - len(shape))])\n",
    "    if len(axis) > 0:\n",
    "        gradient = np.sum(gradient, axis=axis, keepdims=False)\n",
    "\n",
    "    message = (input_operation, current_operation, gradient, gradient.shape, shape)\n",
    "    assert gradient.shape == shape, message\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to go through the numeric test code.\n",
    "\n",
    " - Why do we define gradient functions ourselfs in algorithmic differentiation, if we could just use the numeric gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_test(variable, *, eps=1e-7, rtol=1e-7, atol=0):\n",
    "    assert isinstance(variable, Variable), f'Can only calculate gradients for Graph elements and not for {variable}.'\n",
    "    for arg in variable.operation.args:\n",
    "        if not isinstance(arg, (Input, Parameter)):\n",
    "            if isinstance(arg, Variable):\n",
    "                raise AssertionError(\n",
    "                    'Each arg of the operation has to be an Input or Parameter.\\n'\n",
    "                    'Intermediate variables are not allowed.\\n'\n",
    "                    f'The operation {variable.operation} got the input argument: {arg}.\\n'\n",
    "                    'Change the Variable to an Input or Parameter.'\n",
    "                )\n",
    "            else:\n",
    "                raise AssertionError(\n",
    "                    f'Can only calculate gradients for Graph elements and not for {arg}.'\n",
    "                )        \n",
    "\n",
    "    args = [\n",
    "        np.array(arg.value, dtype=np.float64) \n",
    "        for arg in variable.operation.args\n",
    "    ]\n",
    "\n",
    "    gout = np.random.normal(size=variable.value.shape)\n",
    "    \n",
    "    gin = variable.operation.grad(gout)\n",
    "    if not isinstance(gin, tuple):\n",
    "        raise AssertionError(\n",
    "            f'The grad function of {type(variable.operation)} returned a {type(gin)}.\\n'\n",
    "            f'The grad function should always return a tuple: e.g.: \"return gx,\" or \"return gx, gy\".\\n'\n",
    "            f'Did you forgot to write the comma?'\n",
    "        )\n",
    "    \n",
    "    gin = list(gin)\n",
    "    for i in range(len(gin)):\n",
    "        gin[i] = handle_broadcast(variable.operation.args[i], gin[i], variable.operation)\n",
    "\n",
    "    gin_numeric = [np.zeros_like(g) for g in gin]\n",
    "    for i in range(len(gin)):\n",
    "        # Iterate through every element in every input\n",
    "        for nd_idx in np.ndindex(args[i].shape):\n",
    "            args[i][nd_idx] += eps\n",
    "            out_eps = variable.operation.apply(*args, **variable.operation.kwargs)\n",
    "            gin_numeric[i][nd_idx] = np.sum(gout * (out_eps - variable.value)) / eps\n",
    "            args[i][nd_idx] -= eps\n",
    "\n",
    "    for i in range(len(gin)):\n",
    "        np.testing.assert_allclose(gin[i], gin_numeric[i], rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Parameter(123.)\n",
    "y = Identity(x)\n",
    "numeric_test(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and test each operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start to define a bunch of operations we might need, to reconstruct the theory exercise about algorithmic differentiation.\n",
    "\n",
    "You need to define each `apply()` function and each `grad()` function. You can then run the provided test case to check, if your gradient calculation matches the numerical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "    def apply(self, x, y):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            return ???, ???\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(123.)\n",
    "y = Parameter(-100.)\n",
    "z = x + y\n",
    "np.testing.assert_allclose(z.value, 23.)\n",
    "numeric_test(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(Operation):\n",
    "    def apply(self, x, y):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            return ???, ???\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(123.)\n",
    "y = Parameter(-100.)\n",
    "z = x - y\n",
    "np.testing.assert_allclose(z.value, 223.)\n",
    "numeric_test(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Operation):\n",
    "    def apply(self, x, y):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            return ???, ???\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(123.)\n",
    "y = Parameter(-100.)\n",
    "z = x * y\n",
    "np.testing.assert_allclose(z.value, -12300.)\n",
    "numeric_test(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rectified linear unit is already a tricky beast.\n",
    "It is not differentiable at 0, nevertheless, it is frequently used in todays neural networks.\n",
    "The gradient is implemented in such a way, that the tangent of the curve is always below the curve itself (sometimes called subgradient).\n",
    "In practice, this means that we can assume the that the gradient is $1$ at $x = 0$ without losing anything.\n",
    "\n",
    "Check the difference between `np.max()` and `np.maximum()` to solve this part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Operation):\n",
    "    def apply(self, x):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            gx = gz.copy()\n",
    "            \n",
    "            # Modify `gx` such that the gradient is corrected for x < 0.\n",
    "            ???\n",
    "            return gx,\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(123.)\n",
    "z = ReLU(x)\n",
    "np.testing.assert_allclose(z.value, 123.)\n",
    "numeric_test(z)\n",
    "\n",
    "x = Parameter(-123.)\n",
    "z = ReLU(x)\n",
    "np.testing.assert_allclose(z.value, 0.)\n",
    "numeric_test(z)\n",
    "\n",
    "\n",
    "x = Parameter([-123., 123.])\n",
    "z = ReLU(x)\n",
    "np.testing.assert_allclose(z.value, [0., 123.])\n",
    "numeric_test(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce theory exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce the execution graph of the theory exercise by first defining the inputs as variables using `Parameter` and then adding further graph elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = graphviz.Digraph()\n",
    "dot.attr(rankdir='BT')  # draw from bottom to top\n",
    "dot.node('x', 'x = 7', shape='plaintext')\n",
    "dot.node('gamma', 'gamma = 5', shape='plaintext')\n",
    "dot.node('J', 'J', shape='plaintext')\n",
    "\n",
    "dot.node('relu', 'ReLU', shape='box')\n",
    "dot.node('plus', '+', shape='circle')\n",
    "dot.node('mul', '*', shape='circle')\n",
    "\n",
    "dot.edge('x', 'relu')\n",
    "dot.edge('x', 'mul')\n",
    "dot.edge('gamma', 'mul')\n",
    "dot.edge('relu', 'plus', 'h')\n",
    "dot.edge('mul', 'plus', 'z')\n",
    "dot.edge('plus', 'J')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ???\n",
    "gamma = ???\n",
    "\n",
    "h = ???\n",
    "z = ???\n",
    "J = ???\n",
    "\n",
    "np.testing.assert_allclose(J.value, 42.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get the execution order. Does the execution order match the execution order of the theory exercise?\n",
    "Is this a [Depth-first search](https://en.wikipedia.org/wiki/Depth-first_search) or is it a [Breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search)?\n",
    "\n",
    "Please go through the code and understand, which elements are added to the `all_variables` list first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(variable):\n",
    "    if not isinstance(variable, Variable):\n",
    "        raise TypeError(\n",
    "            f'Expect something of type {Variable} and not of type {type(variable)}. Variable: {variable}'\n",
    "        )\n",
    "    all_variables = []\n",
    "\n",
    "    def recursive_call(variable):\n",
    "        if isinstance(variable, Variable):\n",
    "            if variable.operation is not None:\n",
    "                for input_variable in variable.operation.args:\n",
    "                    if input_variable not in all_variables:\n",
    "                        recursive_call(input_variable)\n",
    "            all_variables.append(variable)\n",
    "\n",
    "    recursive_call(variable)\n",
    "    return all_variables\n",
    "\n",
    "get_variables(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we are able to obtain the execution order (i.e. the operation that produced the variable value), it may be more fun to visualize the calculations within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(variable):\n",
    "    all_variable = get_variables(variable)\n",
    "\n",
    "    dg = graphviz.Digraph()\n",
    "    dg.attr(rankdir='BT')  # draw from bottom to top\n",
    "    dg.attr(size='6')\n",
    "    for variable in all_variable:\n",
    "        # dg.node(str(id(variable)), str(variable), shape='box')\n",
    "        dg.node(str(id(variable)), str(variable), shape='plaintext')\n",
    "        \n",
    "        if variable.operation is not None:\n",
    "            dg.node(str(id(variable.operation)), str(variable.operation))\n",
    "            dg.edge(str(id(variable.operation)), str(id(variable)))\n",
    "            for input_variable in variable.operation.args:\n",
    "                edge_label = None\n",
    "                dg.edge(str(id(input_variable)), str(id(variable.operation)), label=edge_label)\n",
    "    return dg\n",
    "\n",
    "visualize(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the computation graph we can now calculate the gradients.\n",
    "Algorithmic differentiation in reverse mode is also known as backpropagation.\n",
    "\n",
    "Try to understand how the backpropagation code works.\n",
    "\n",
    " - Why is `map_variable_to_gradient` initialized with zeros?\n",
    " - And why is the returned gradient from `operation.grad(...)` added to the old value?\n",
    " - In which order do we call `operation.grad(...)`?\n",
    " - Do you know the Multivariable chain rule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(loss):\n",
    "    assert loss.value.shape == (), f'The loss should be a scalar. shape {loss.value.shape}'\n",
    "    \n",
    "    map_variable_to_gradient = OrderedDict()\n",
    "    for variable in reversed(get_variables(loss)):\n",
    "        map_variable_to_gradient[variable] = np.array(0.)\n",
    "\n",
    "    map_variable_to_gradient[loss] = np.array(1.)\n",
    "    \n",
    "    for variable in map_variable_to_gradient.keys():\n",
    "        operation = variable.operation\n",
    "        if operation is not None:\n",
    "            gradients = operation.grad(map_variable_to_gradient[variable])\n",
    "\n",
    "            message = (operation, type(gradients), gradients)\n",
    "            assert isinstance(gradients, tuple), message\n",
    "            message = (len(gradients), len(operation.args))\n",
    "            assert len(gradients) == len(operation.args), message\n",
    "\n",
    "            for input_argument, gradient in zip(operation.args, gradients):\n",
    "                input_variable = input_argument\n",
    "                map_variable_to_gradient[input_variable] = np.array(\n",
    "                        map_variable_to_gradient[input_variable]\n",
    "                        + handle_broadcast(\n",
    "                            input_operation=input_variable,\n",
    "                            gradient=np.array(gradient),\n",
    "                            current_operation=operation,\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return map_variable_to_gradient\n",
    "backprop(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce theory exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = graphviz.Digraph()\n",
    "dot.attr(rankdir='BT')  # draw from bottom to top\n",
    "dot.node('x', 'x = 5', shape='plaintext')\n",
    "dot.node('w', 'w=2', shape='plaintext')\n",
    "dot.node('b', 'b=-1', shape='plaintext')\n",
    "dot.node('y', 'y=7', shape='plaintext')\n",
    "dot.node('J', 'J', shape='plaintext')\n",
    "\n",
    "dot.node('mul', '*', shape='circle')\n",
    "dot.node('plus1', '+', shape='circle')\n",
    "dot.node('relu', 'ReLU', shape='box')\n",
    "dot.node('plus2', '+', shape='circle')\n",
    "dot.node('mse', 'MSE', shape='box')\n",
    "\n",
    "dot.edge('x', 'mul')\n",
    "dot.edge('w', 'mul')\n",
    "dot.edge('mul', 'plus1', 'v')\n",
    "dot.edge('b', 'plus1')\n",
    "dot.edge('plus1', 'relu', 'z')\n",
    "dot.edge('relu', 'plus2', 'h')\n",
    "dot.edge('plus1', 'plus2', 'z')\n",
    "dot.edge('plus2', 'mse', 'y hat')\n",
    "dot.edge('y', 'mse')\n",
    "dot.edge('mse', 'J')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excercises contain two examples for algorithmic differentation. The second example (Excercise 4.1, Homework) contains a mean square error (MSE).\n",
    "\n",
    "Reproduce the execution graph of that theory exercise by first defining the MSE operation.\n",
    "Instantiate the inputs as parameters using `Parameter` and then adding further graph elements.\n",
    "\n",
    " - How is the operation mean square error (MSE) defined?\n",
    " - What is the derivative of MSE with respect to the inputs y and y hat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Operation):\n",
    "    def apply(self, x, y):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            gz = np.broadcast_to(gz, x.shape)\n",
    "            gx = ???\n",
    "            gy = ???\n",
    "            return gx, gy\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(1.)\n",
    "y = Parameter(-1.)\n",
    "z = MSE(x, y)\n",
    "np.testing.assert_allclose(z.value, 4.)\n",
    "numeric_test(z, eps=1e-10)\n",
    "\n",
    "x = Parameter(np.ones(4))\n",
    "y = Parameter([1, 1, 1, 0.])\n",
    "z = MSE(x, y)\n",
    "np.testing.assert_allclose(z.value, 0.25)\n",
    "numeric_test(z, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ???\n",
    "w = ???\n",
    "b = ???\n",
    "y = ???\n",
    "\n",
    "v = ???\n",
    "z = ???\n",
    "h = ???\n",
    "y_hat = ???\n",
    "J = ???\n",
    "\n",
    "np.testing.assert_allclose(J.value, 49.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define more operations and symbolic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Operation):\n",
    "    def apply(self, x, axis=None, keepdims=False):\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            if axis is None:\n",
    "                return np.broadcast_to(gz, x.shape),\n",
    "            \n",
    "            if keepdims:\n",
    "                gx = np.reshape(gz, z.shape)\n",
    "                return np.broadcast_to(gx, x.shape),\n",
    "            else:\n",
    "                shape = list(x.shape)\n",
    "                shape[axis] = 1\n",
    "                gx = np.reshape(gz, shape)\n",
    "                return np.broadcast_to(gx, x.shape),\n",
    "        \n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(1.)\n",
    "z = Sum(x)\n",
    "np.testing.assert_allclose(z.value, 1.)\n",
    "numeric_test(z)\n",
    "\n",
    "x = Parameter(np.ones((3, 4)))\n",
    "z = Sum(x)\n",
    "np.testing.assert_allclose(z.value, 12.)\n",
    "numeric_test(z)\n",
    "\n",
    "x = Parameter(np.ones((3, 4)))\n",
    "z = Sum(x, axis=1)\n",
    "np.testing.assert_allclose(z.value, 4 * np.ones(3))\n",
    "numeric_test(z, eps=1-7)\n",
    "\n",
    "x = Parameter(np.ones((3, 4)))\n",
    "z = Sum(x, axis=1, keepdims=True)\n",
    "np.testing.assert_allclose(z.value, 4 * np.ones((3, 1)))\n",
    "numeric_test(z)\n",
    "visualize(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pow(Operation):\n",
    "    def apply(self, x, y):\n",
    "        z = x ** y\n",
    "\n",
    "        def grad(gz):\n",
    "            gx = gz * y * (x ** (y - 1))\n",
    "            gy = gz * np.log(x) * z\n",
    "            return gx, gy\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(2.)\n",
    "y = Parameter(2.)\n",
    "z = x ** y\n",
    "np.testing.assert_allclose(z.value, 4.)\n",
    "numeric_test(z, eps=1e-10, rtol=1e-6, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we defined a lot of elemantary operations, we can also compose them to larger building blocks. That way, we do not need to define the gradients anymore. Define the MSE function using only the predefined operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_MSE(x, y):\n",
    "    return ???\n",
    "\n",
    "x = Parameter(1.)\n",
    "y = Parameter(-1.)\n",
    "z = symbolic_MSE(x, y)\n",
    "np.testing.assert_allclose(z.value, 4.)\n",
    "\n",
    "x = Parameter(np.ones(4))\n",
    "y = Parameter([1, 1, 1, 0])\n",
    "z = symbolic_MSE(x, y)\n",
    "np.testing.assert_allclose(z.value, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the complete graph from the second theoretical exercise again using the `symbolic_MSE` function. What changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ???\n",
    "w = ???\n",
    "b = ???\n",
    "y = ???\n",
    "\n",
    "v = ???\n",
    "z = ???\n",
    "h = ???\n",
    "y_hat = ???\n",
    "J = ???\n",
    "\n",
    "np.testing.assert_allclose(J.value, 49.)\n",
    "visualize(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the end of the exercise.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for the homework assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the homework assignment, we need to define more operations. The matrix multiplication gradient is already everything but trivial. If you are interested, you can check the implementation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(Operation):\n",
    "    def apply(self, x, y):\n",
    "\n",
    "        z = ???\n",
    "\n",
    "        def grad(gz):\n",
    "            if x.ndim > 1 and y.ndim > 1:\n",
    "                return gz @ np.swapaxes(y, -2, -1), np.swapaxes(x, -2, -1) @ gz\n",
    "            elif x.ndim == 1 and y.ndim > 1:\n",
    "                return gz @ np.swapaxes(y, -2, -1), np.einsum('i,...j->...ij', x, gz)\n",
    "            elif x.ndim > 1 and y.ndim == 1:\n",
    "                return np.einsum('...i,j->...ij', gz, y), np.swapaxes(x, -2, -1) @ gz\n",
    "            elif x.ndim == 1 and y.ndim == 1:\n",
    "                return np.einsum(',j->j', gz, y), np.einsum('i,->i', x, gz)\n",
    "            else:\n",
    "                raise NotADirectoryError(x.ndim, y.ndim)\n",
    "        self.grad = grad\n",
    "\n",
    "        return z\n",
    "\n",
    "x = Parameter(np.random.normal(size=[2, 4]))\n",
    "y = Parameter(np.random.normal(size=[4, 3]))\n",
    "z = x @ y\n",
    "numeric_test(z, eps=1e-1)\n",
    "\n",
    "x = Parameter(np.random.normal(size=[4]))\n",
    "y = Parameter(np.random.normal(size=[4, 3]))\n",
    "z = x @ y\n",
    "numeric_test(z, eps=1e-1)\n",
    "\n",
    "x = Parameter(np.random.normal(size=[2, 4]))\n",
    "y = Parameter(np.random.normal(size=[4]))\n",
    "z = x @ y\n",
    "numeric_test(z, eps=1e-1)\n",
    "\n",
    "x = Parameter(np.random.normal(size=[4]))\n",
    "y = Parameter(np.random.normal(size=[4]))\n",
    "z = x @ y\n",
    "numeric_test(z, eps=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know how to combine graph elements, we can also define layers or complete models. Here are two different ways, how to define an affine layer (sometimes called feed-forward layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AffineLayer(in_units, out_units):\n",
    "    small_value = 0.01\n",
    "    W = Parameter(\n",
    "        np.random.uniform(\n",
    "            size=[in_units, out_units],\n",
    "            low=-small_value,\n",
    "            high=small_value\n",
    "        )\n",
    "    )\n",
    "    b = Parameter(np.zeros(shape=out_units))\n",
    "\n",
    "    def affine_layer(x):\n",
    "        return (x @ W) + b\n",
    "\n",
    "    # Not nessesary\n",
    "    affine_layer.W = W\n",
    "    affine_layer.b = b\n",
    "    \n",
    "    return affine_layer\n",
    "\n",
    "affine_layer = AffineLayer(3, 3)\n",
    "x = Parameter(np.random.normal(size=3))\n",
    "z = affine_layer(x)\n",
    "z.value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer:\n",
    "    def __init__(self, in_units, out_units):\n",
    "        small_value = 0.01\n",
    "        self.W = Parameter(\n",
    "            np.random.uniform(\n",
    "                size=[in_units, out_units],\n",
    "                low=-small_value,\n",
    "                high=small_value\n",
    "            )\n",
    "        )\n",
    "        self.b = Parameter(np.zeros(shape=out_units))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return (x @ self.W) + self.b\n",
    "\n",
    "affine_layer = AffineLayer(3, 3)\n",
    "x = Parameter(np.random.normal(size=3))\n",
    "z = affine_layer(x)\n",
    "z.value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When different layers are combined, often a `Sequential` class is used.\n",
    "Here is an example implementation of such a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        hidden_state = input\n",
    "        for layer in self.layers:\n",
    "            hidden_state = layer(hidden_state)\n",
    "        return hidden_state\n",
    "    \n",
    "Sequential(AffineLayer(3, 3), AffineLayer(3, 10))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming, your Jupyter notebook has internet connection, we can already download the MNIST dataset and look at the data structure. If you work on [cocalc.com](https://cocalc.com/), you will not have an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    # The code to download the mnist data original came from\n",
    "    # https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html\n",
    "    \n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    from urllib.request import urlretrieve \n",
    "\n",
    "    def load_data(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x3080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if n != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
    "                    )\n",
    "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if crow != 28 or ccol != 28:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
    "                    )\n",
    "                # Read data.\n",
    "                res = np.frombuffer(\n",
    "                    gz.read(num_samples * crow * ccol), dtype=np.uint8\n",
    "                )\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples, crow, ccol)) / 256\n",
    "\n",
    "\n",
    "    def load_labels(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x1080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))\n",
    "                if n[0] != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
    "                    )\n",
    "                # Read labels.\n",
    "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples))\n",
    "\n",
    "\n",
    "    def try_download(data_source, label_source, num_samples):\n",
    "        data = load_data(data_source, num_samples)\n",
    "        labels = load_labels(label_source, num_samples)\n",
    "        return data, labels\n",
    "    \n",
    "    # Not sure why, but yann lecun's website does no longer support \n",
    "    # simple downloader. (e.g. urlretrieve and wget fail, while curl work)\n",
    "    # Since not everyone has linux, use a mirror from uni server.\n",
    "    #     server = 'http://yann.lecun.com/exdb/mnist'\n",
    "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master'\n",
    "    \n",
    "    # URLs for the train image and label data\n",
    "    url_train_image = f'{server}/train-images-idx3-ubyte.gz'\n",
    "    url_train_labels = f'{server}/train-labels-idx1-ubyte.gz'\n",
    "    num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data\")\n",
    "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "    # URLs for the test image and label data\n",
    "    url_test_image = f'{server}/t10k-images-idx3-ubyte.gz'\n",
    "    url_test_labels = f'{server}/t10k-labels-idx1-ubyte.gz'\n",
    "    num_test_samples = 10000\n",
    "\n",
    "    print(\"Downloading test data\")\n",
    "    test_features, test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "    \n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_train, feature_test = np.split(mnist['data'], [60000])\n",
    "# label_train, label_test = np.split(mnist['target'], [60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_features[6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a gradient from the dictionary:\n",
    "\n",
    "mu = -0.01\n",
    "\n",
    "print('Manual')\n",
    "x = Input(np.array([1.]))\n",
    "J = (x * x).sum()\n",
    "gradient_dict = backprop(J)\n",
    "g = gradient_dict[x]\n",
    "print(x.value)\n",
    "x.value += mu * g\n",
    "print(x.value)\n",
    "\n",
    "# or:\n",
    "\n",
    "print('Manual with layers')\n",
    "x = Input(np.array([1.]))\n",
    "dense_layer = AffineLayer(1, 1)\n",
    "J = dense_layer(x).sum()\n",
    "gradient_dict = backprop(J)\n",
    "g = gradient_dict[dense_layer.W]\n",
    "print(dense_layer.W.value)\n",
    "dense_layer.W.value += mu * g\n",
    "print(dense_layer.W.value)\n",
    "\n",
    "# or:\n",
    "\n",
    "print('Automatic')\n",
    "for k, g in gradient_dict.items():\n",
    "    if isinstance(k, Parameter):\n",
    "        print(k.value)\n",
    "        k.value += mu * g\n",
    "        print(k.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters\n",
    "\n",
    "def dump_parameters(loss, file):\n",
    "    variables = get_variables(J)\n",
    "    parameters = [\n",
    "        v \n",
    "        for v in variables\n",
    "        if isinstance(v, Parameter)\n",
    "    ]\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump([p.value.tolist() for p in parameters], fp)\n",
    "    print('Wrote the parameters')\n",
    "    print(parameters)\n",
    "    print('to', file)\n",
    "\n",
    "dump_parameters(loss=J, file='parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the parameters\n",
    "\n",
    "def load_parameters(loss, file):\n",
    "    variables = get_variables(J)\n",
    "    parameters = [\n",
    "        v \n",
    "        for v in variables\n",
    "        if isinstance(v, Parameter)\n",
    "    ]\n",
    "    with open(file) as fp:\n",
    "        parameters_values = json.load(fp)\n",
    "    for p, p_value in zip(parameters, parameters_values):\n",
    "        print(p.value, p_value)\n",
    "        p.value[...] = p_value\n",
    "        \n",
    "    print('Loaded the parameters')\n",
    "    print(parameters)\n",
    "    print('from', file)\n",
    "        \n",
    "load_parameters(loss=J, file='parameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate the correct python enviroment\n",
    "https://fgnt.github.io/python_crashkurs_doc/include/poolroom.html\n",
    "```bash\n",
    "source /upb/scratch/users/c/cbj/py37/bin/activate\n",
    "```\n",
    "Remember: cocalc is slow and not recommended for the homework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
