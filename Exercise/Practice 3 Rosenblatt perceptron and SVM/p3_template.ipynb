{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = True  # Use per default a grid, i.e. plt.grid()\n",
    "# mpl.rcParams['figure.figsize'] = [6.4, 4.8]  # Change the default figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.set_printoptions:\n",
    "#     threshold: Total number of array elements which trigger summarization rather than full repr (default 1000).\n",
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_0_1_to_minus_1_1(labels):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    return 2 * labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    features: np.array\n",
    "    labels: np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circles_dataset(number_of_samples=1000, random_state=np.random):\n",
    "    labels = random_state.choice([0, 1], size=number_of_samples)\n",
    "    features = random_state.normal(size=(number_of_samples, 2))\n",
    "    features /= np.linalg.norm(features, axis=-1, keepdims=True)\n",
    "    features *= labels[:, None] + 1 / 2 * random_state.uniform(size=number_of_samples)[:, None]\n",
    "    return Dataset(features, map_0_1_to_minus_1_1(labels))\n",
    "\n",
    "def generate_blobs_dataset(number_of_samples=1000, scale=1, random_state=np.random):\n",
    "    labels = random_state.choice([0, 1], size=number_of_samples)\n",
    "    cluster_centers = random_state.normal(scale=scale, size=(2, 2))\n",
    "    features = cluster_centers[labels, :] + random_state.normal(size=(number_of_samples, 2))\n",
    "    return Dataset(features, map_0_1_to_minus_1_1(labels))\n",
    "\n",
    "def generate_checkerboard_dataset(number_of_samples=1000, random_state=np.random):\n",
    "    features = random_state.uniform(-1, 1, size=(number_of_samples, 2))\n",
    "    labels = np.sign(np.prod(features, axis=1)).astype(np.int)\n",
    "    return Dataset(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = generate_circles_dataset(random_state=np.random.RandomState(0))\n",
    "easy_blobs = generate_blobs_dataset(scale=4, random_state=np.random.RandomState(0))\n",
    "checkerboard = generate_checkerboard_dataset(random_state=np.random.RandomState(0))\n",
    "hard_blobs = generate_blobs_dataset(scale=1, random_state=np.random.RandomState(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the datasets are linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    classes = np.unique(dataset.labels)\n",
    "    classes = np.sort(classes)\n",
    "    for c in classes:\n",
    "        ax.scatter(\n",
    "            dataset.features[dataset.labels == c, 0],\n",
    "            dataset.features[dataset.labels == c, 1],\n",
    "            label=f'Class: {c}',\n",
    "        )\n",
    "    ax.set_aspect('equal')\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "figure, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "plot_dataset(circles, ax=axes[0, 0], title='circles')\n",
    "plot_dataset(easy_blobs, ax=axes[0, 1], title='easy_blobs')\n",
    "plot_dataset(checkerboard, ax=axes[1, 0], title='checkerboard')\n",
    "plot_dataset(hard_blobs, ax=axes[1, 1], title='hard_blobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a shorter notation, lets use the following abbreviations:\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{w}} &= \\begin{pmatrix}\\mathbf w \\\\ w_0\\end{pmatrix}, &\n",
    "\\tilde{\\mathbf{x}}_n &= \\begin{pmatrix}\\mathbf x_n \\\\ 1\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Apply the Rosenblatt criterion to each of the datasets:\n",
    "\\begin{align}\n",
    "J(\\tilde{\\mathbf{w}}) &= -\\frac 1 N \\sum_{n=1}^N \\left(\\frac{c_n - \\hat c_n}{2}\\right) \\tilde{\\mathbf{w}}^{\\mathsf T} \\tilde{\\mathbf{x}}_n\n",
    "\\end{align}\n",
    "\n",
    "Make sure to use some stopping criterion to avoid infinite loops. Plot the training loss over iterations. For which of the datasets does the training converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_augmentation(x):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \n",
    "    Augmented vector: https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix\n",
    "    \"\"\"\n",
    "    return ???\n",
    "\n",
    "def remove_augmentation(x_tilde):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rosenblatt_perceptron(x_tilde, w_tilde, transform_fn=None):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    if transform_fn is not None:\n",
    "        x = remove_augmentation(x_tilde)\n",
    "        x = transform_fn(x)\n",
    "        x_tilde = add_augmentation(x)\n",
    "    \n",
    "    discriminant = ???\n",
    "    prediction = ???\n",
    "    return discriminant, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rosenblatt_perceptron(dataset, iterations=1000, learning_rate=0.1, transform_fn=None):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    loss_history = np.empty(iterations)\n",
    "    loss_history[:] = np.nan\n",
    "    N = dataset.features.shape[0]\n",
    "    \n",
    "    if transform_fn is None:\n",
    "        x_tilde = add_augmentation(dataset.features)\n",
    "    else:\n",
    "        x = transform_fn(dataset.features)\n",
    "        x_tilde = add_augmentation(x)\n",
    "    \n",
    "    w_tilde = np.random.normal(size=x_tilde.shape[-1])\n",
    "    c = dataset.labels\n",
    "    for iteration in range(iterations):\n",
    "        _, c_hat = predict_rosenblatt_perceptron(x_tilde, w_tilde)\n",
    "        loss_history[iteration] = ???\n",
    "        w_tilde = ???\n",
    "    return w_tilde, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complete_grid(xlim, ylim, steps):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    x, y = np.meshgrid(\n",
    "        np.linspace(*xlim, steps),\n",
    "        np.linspace(*ylim, steps),\n",
    "    )\n",
    "    return x, y, np.stack([x, y], axis=-1)\n",
    "\n",
    "def fit_and_plot(dataset, title, transform_fn=None, fit_function=fit_rosenblatt_perceptron, **kwargs):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    w_tilde, loss_history = fit_function(\n",
    "        dataset,\n",
    "        transform_fn=transform_fn,\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].plot(loss_history)\n",
    "    axes[0].set_title('{}, final loss: {}'.format(title, loss_history[-1]))\n",
    "    # Use log scale above 1e-5 and linear scale below\n",
    "    axes[0].set_yscale(\"symlog\", linthreshy=1e-5)\n",
    "    plot_dataset(dataset, ax=axes[1], title=title)\n",
    "    \n",
    "    # Plot decision boundary if in area\n",
    "    steps = 100\n",
    "    x, y, features_grid = compute_complete_grid(\n",
    "        (np.min(dataset.features[:, 0]), np.max(dataset.features[:, 0])),\n",
    "        (np.min(dataset.features[:, 1]), np.max(dataset.features[:, 1])),\n",
    "        steps=steps\n",
    "    )\n",
    "    features_grid = features_grid.reshape(steps * steps, 2)\n",
    "    features_grid = add_augmentation(features_grid)\n",
    "    z, _ = predict_rosenblatt_perceptron(features_grid, w_tilde, transform_fn=transform_fn)\n",
    "    z = np.reshape(z, (steps, steps))\n",
    "    if np.any(z < 0) and np.any(z > 0):\n",
    "        axes[1].contour(x, y, z, levels=[0])\n",
    "    else:\n",
    "        axes[1].contour(x, y, z)\n",
    "    plt.show()\n",
    "    \n",
    "    print('w_tilde', w_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which datasets are linearly separable?\n",
    "- Explain the loss curve for the non linearly separable datasets.\n",
    "- Is the loss curve an indicator if the we can get a reasonable discriminant for non linearly separable datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(circles, 'circles')\n",
    "fit_and_plot(easy_blobs, 'easy_blobs')\n",
    "fit_and_plot(checkerboard, 'checkerboard')\n",
    "fit_and_plot(hard_blobs, 'hard_blobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation\n",
    "\n",
    "Which hand-crafted feature transformation is necessary to render the `circles` and `checkerboard` datasets linearly separable? Train a Rosenblatt perceptron using this transformation. Plot the trainig loss over iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyse the decision boundarys.\n",
    "- Which solution generalizes better?\n",
    "- For which dataset do you expect a better solution from SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    circles,\n",
    "    'circles',\n",
    "    transform_fn=lambda x: ???\n",
    ")\n",
    "fit_and_plot(\n",
    "    checkerboard,\n",
    "    'checkerboard',\n",
    "    iterations=10000,\n",
    "    transform_fn=lambda x: ???\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "\n",
    "Train an SVM using gradient descent. For which datasets does the algorithm converge?\n",
    "\n",
    "\\begin{align}\n",
    "\t\\tilde{\\mathcal{L}}(\\mathbf{w},w_0) = \\frac{\\|\\mathbf{w}\\|^2}{2} + \\frac C N \\sum_{n=1}^N \\max\\left(0, 1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0) \\right).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Text from SML script to this equation (Eq. 4.36):\n",
    "\n",
    "> The first term will push the model to have a small weight vector $\\mathbf w$, leading to a large\n",
    "margin, while the second term computes the total of all margin violations. Minimizing\n",
    "this term ensures that the model makes the margin violations as small and as few as\n",
    "possible. $C$ is a trade-off parameter between the two contributions to the cost function.\n",
    "This objective function can be minimized by gradient descent.\n",
    ">\n",
    "> By the way, the function max(0, 1−x) is the hinge loss we have seen earlier in Section 2.7.3\n",
    "\n",
    "Apply the same feature transform as before to render the `circles` and `checkerboard` dataset linearly separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\mathbf w} &= \\mathbf w + \\frac C N \\sum_{n=1}^N -c_n \\mathrm{heaviside}\\bigg(1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0)\\bigg) \\mathbf x_n \\\\\n",
    "\\frac{\\partial J}{\\partial w_0} &= \\frac C N \\sum_{n=1}^N -c_n \\mathrm{heaviside}\\bigg(1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0)\\bigg)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_support_vector_machine(dataset, transform_fn=None, iterations=10000, learning_rate=0.01, hinge_loss_weight=10):\n",
    "    loss_history = np.empty(iterations)\n",
    "    loss_history[:] = np.nan\n",
    "    N = dataset.features.shape[0]\n",
    "    x_tilde = add_augmentation(dataset.features)\n",
    "    \n",
    "    if transform_fn is not None:\n",
    "        x = remove_augmentation(x_tilde)\n",
    "        x = transform_fn(x)\n",
    "        x_tilde = add_augmentation(x)\n",
    "    \n",
    "    w_tilde = np.random.normal(size=x_tilde.shape[-1])\n",
    "    c = dataset.labels\n",
    "    for iteration in range(iterations):\n",
    "        discriminant, c_hat = predict_rosenblatt_perceptron(x_tilde, w_tilde)\n",
    "        loss_history[iteration] = ???\n",
    "        gradient_w = ???\n",
    "        gradient_w_0 = ???\n",
    "        gradient = np.concatenate((gradient_w, [gradient_w_0]))\n",
    "        w_tilde = w_tilde - learning_rate * gradient\n",
    "    return w_tilde, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(easy_blobs, 'easy_blobs', fit_function=fit_support_vector_machine, hinge_loss_weight=1e4, learning_rate=0.001)\n",
    "fit_and_plot(hard_blobs, 'hard_blobs', fit_function=fit_support_vector_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(circles, 'circles', fit_function=fit_support_vector_machine, transform_fn=lambda x: np.linalg.norm(x, axis=-1, keepdims=True))\n",
    "fit_and_plot(\n",
    "    checkerboard, 'checkerboard',\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    transform_fn=lambda x: ???,\n",
    "    iterations=???,\n",
    "    learning_rate=???,\n",
    "    hinge_loss_weight=???,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dataset = Dataset(np.array([[1, 1], [-1, 1], [1, -1], [-1, -1]]) * 100, np.array([-1, -1, 1, 1]))\n",
    "plot_dataset(sign_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should the decision boundary look like?\n",
    "- Can you explain, why the SVM code does not work for this simple example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(sign_dataset, 'sign', fit_function=fit_support_vector_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dataset2 = Dataset(np.array([[1, 1], [-1, 1], [0, -1], [0, -1.5]]), np.array([-1, -1, 1, 1]))\n",
    "plot_dataset(sign_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should the decision boundary look like?\n",
    "- Can you explain, why the decision boundary is not the expected one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(sign_dataset2, 'sign2', fit_function=fit_support_vector_machine, hinge_loss_weight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For experts...\n",
    "\n",
    "- The SVM parameters can be learned faster by using an appropriate solver instead of using gradient descent.\n",
    "Check how the parameters are obtained in the `sklearn` source code and use it to verify your results.\n",
    "- Mark the support vectors in your diagrams.\n",
    "- What is the advantage of training the SVM with gradient descent?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
